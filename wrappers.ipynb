{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec225780-a6e1-4fd2-98ef-a51cd13ff1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom Gymnasium wrappers for Ms. Pac-Man\n",
    "\"\"\"\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RawRewardTracker(gym.Wrapper):\n",
    "    \"\"\"Track raw rewards before any transformation\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.episode_raw_rewards = []\n",
    "        self.cumulative_raw_reward = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset episode tracking\"\"\"\n",
    "        self.episode_raw_rewards = []\n",
    "        self.cumulative_raw_reward = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Step environment and track raw reward\"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Store raw reward BEFORE any shaping\n",
    "        self.episode_raw_rewards.append(reward)\n",
    "        self.cumulative_raw_reward += reward\n",
    "        \n",
    "        # Add to info at episode end\n",
    "        if terminated or truncated:\n",
    "            info['raw_episode_return'] = self.cumulative_raw_reward\n",
    "            info['raw_rewards_list'] = self.episode_raw_rewards.copy()\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class MetricsWrapper(gym.Wrapper):\n",
    "    \"\"\"Wrapper that tracks RAM state and calculates metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, env, x_byte, y_byte, pellet_byte):\n",
    "        super().__init__(env)\n",
    "        self.x_byte = x_byte\n",
    "        self.y_byte = y_byte\n",
    "        self.pellet_byte = pellet_byte\n",
    "        \n",
    "        # Episode tracking\n",
    "        self.episode_positions = []\n",
    "        self.episode_steps = 0\n",
    "        self.episode_pellets_start = 0\n",
    "        self.current_level = 1\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset episode tracking\"\"\"\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        \n",
    "        self.episode_positions = []\n",
    "        self.episode_steps = 0\n",
    "        \n",
    "        # Get RAM state\n",
    "        if isinstance(obs, tuple):\n",
    "            ram_state = obs[0]\n",
    "        else:\n",
    "            ram_state = obs\n",
    "        \n",
    "        self.episode_pellets_start = int(ram_state[self.pellet_byte])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Step environment and track metrics\"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Store position (from RAM)\n",
    "        x = int(obs[self.x_byte])\n",
    "        y = int(obs[self.y_byte])\n",
    "        self.episode_positions.append((x, y))\n",
    "        \n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Detect level completion\n",
    "        current_pellets = int(obs[self.pellet_byte])\n",
    "        \n",
    "        # If pellet count increased (new level started)\n",
    "        if current_pellets > self.episode_pellets_start:\n",
    "            self.current_level += 1\n",
    "            self.episode_pellets_start = current_pellets  # FIX: Update baseline\n",
    "        \n",
    "        # Get raw rewards from RawRewardTracker (in info)\n",
    "        raw_rewards = info.get('raw_rewards_list', [])\n",
    "        \n",
    "        # Calculate metrics at episode end\n",
    "        if terminated or truncated:\n",
    "            metrics = self.calculate_metrics(raw_rewards)\n",
    "            info['metrics'] = metrics\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def calculate_metrics(self, raw_rewards):\n",
    "        \"\"\"Calculate all metrics for the episode\"\"\"\n",
    "        \n",
    "        # 1. Average Lifetime\n",
    "        lifetime = self.episode_steps\n",
    "        \n",
    "        # 2. Pellet Efficiency (from RAW rewards)\n",
    "        pellets_eaten = sum(1 for r in raw_rewards if r == 10)\n",
    "        pellet_efficiency = pellets_eaten / lifetime if lifetime > 0 else 0\n",
    "        \n",
    "        # 3. Ghost-Eating Efficiency (from RAW rewards)\n",
    "        power_pellets_eaten = sum(1 for r in raw_rewards if r == 50)\n",
    "        \n",
    "        ghosts_eaten = 0\n",
    "        frightened_mode = False\n",
    "        frightened_timer = 0\n",
    "        \n",
    "        for r in raw_rewards:\n",
    "            # Enter frightened mode when power pellet eaten\n",
    "            if r == 50:\n",
    "                frightened_mode = True\n",
    "                frightened_timer = 0\n",
    "            \n",
    "            # Count ghosts (unambiguous rewards)\n",
    "            if r in [400, 800, 1600]:\n",
    "                ghosts_eaten += 1\n",
    "            # For 200 points, only count during frightened mode\n",
    "            elif r == 200 and frightened_mode:\n",
    "                ghosts_eaten += 1\n",
    "            \n",
    "            # Frightened mode lasts ~40 steps\n",
    "            if frightened_mode:\n",
    "                frightened_timer += 1\n",
    "                if frightened_timer > 40:\n",
    "                    frightened_mode = False\n",
    "        \n",
    "        ghost_efficiency = ghosts_eaten / power_pellets_eaten if power_pellets_eaten > 0 else 0\n",
    "        \n",
    "        # 4. Backtracking Rate\n",
    "        backtrack_count = 0\n",
    "        visited_positions = set()\n",
    "        \n",
    "        for pos in self.episode_positions:\n",
    "            if pos in visited_positions:\n",
    "                backtrack_count += 1\n",
    "            visited_positions.add(pos)\n",
    "        \n",
    "        backtrack_rate = backtrack_count / lifetime if lifetime > 0 else 0\n",
    "        \n",
    "        # 5. External (raw) reward\n",
    "        external_reward = sum(raw_rewards) if raw_rewards else 0\n",
    "        \n",
    "        return {\n",
    "            'lifetime': lifetime,\n",
    "            'pellet_efficiency': pellet_efficiency,\n",
    "            'ghost_eating_efficiency': ghost_efficiency,\n",
    "            'backtracking_rate': backtrack_rate,\n",
    "            'max_level_reached': self.current_level,\n",
    "            'external_reward': external_reward,\n",
    "            # Additional info\n",
    "            'pellets_eaten': pellets_eaten,\n",
    "            'power_pellets_eaten': power_pellets_eaten,\n",
    "            'ghosts_eaten': ghosts_eaten,\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
