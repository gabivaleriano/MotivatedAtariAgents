{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9751ad8-605f-48a8-bafb-5a15bc227216",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dqn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdqn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DQN_RAM\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menvironment\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_env_with_metrics\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReplayBuffer, set_seed\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dqn'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training functions for DQN on Ms. Pac-Man\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dqn import DQN_RAM\n",
    "from environment import make_env_with_metrics\n",
    "from utils import ReplayBuffer, set_seed\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def train_with_seed(env_name, seed, total_steps=1_000_000, save_dir='results'):\n",
    "    \"\"\"Train one agent with a specific seed\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with seed {seed}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Set seed using utils function\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Create environment\n",
    "    env = make_env_with_metrics(env_name, use_ram=True)\n",
    "    \n",
    "    # Create networks\n",
    "    net = DQN_RAM(env.action_space.n).to(device)\n",
    "    target = DQN_RAM(env.action_space.n).to(device)\n",
    "    target.load_state_dict(net.state_dict())\n",
    "    \n",
    "    # Optimizer and buffer\n",
    "    opt = optim.Adam(net.parameters(), lr=6.25e-5, eps=0.01/32)\n",
    "    buf = ReplayBuffer()\n",
    "    eps = lambda t: 0.1 + 0.9 * np.exp(-t / 500000)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    # Metrics storage\n",
    "    all_metrics = []\n",
    "    rewards_history = []\n",
    "    episode_count = 0\n",
    "    \n",
    "    bar = tqdm(total=total_steps, desc=f\"Seed {seed}\")\n",
    "    \n",
    "    for t in range(1, total_steps + 1):\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() < eps(t):\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q = net(torch.tensor(state.__array__(), device=device).unsqueeze(0))\n",
    "                a = q.argmax(1).item()\n",
    "        \n",
    "        # Environment step\n",
    "        ns, r, term, trunc, info = env.step(a)\n",
    "        done = term or trunc\n",
    "        \n",
    "        # Store experience\n",
    "        buf.push(state.__array__(), a, r, ns.__array__(), done)\n",
    "        state = ns\n",
    "        bar.update(1)\n",
    "        \n",
    "        # Episode ended\n",
    "        if done:\n",
    "            rewards_history.append(info[\"episode\"][\"r\"])\n",
    "            \n",
    "            # Store metrics\n",
    "            if 'metrics' in info:\n",
    "                metrics = info['metrics']\n",
    "                metrics['episode'] = episode_count\n",
    "                metrics['timestep'] = t\n",
    "                metrics['seed'] = seed\n",
    "                all_metrics.append(metrics)\n",
    "            \n",
    "            episode_count += 1\n",
    "            state, _ = env.reset()\n",
    "        \n",
    "        # Training step\n",
    "        if len(buf) >= 32:\n",
    "            s, a_batch, r_batch, ns, d = buf.sample(32)\n",
    "            s = torch.tensor(s, device=device)\n",
    "            ns = torch.tensor(ns, device=device)\n",
    "            a_batch = torch.tensor(a_batch, device=device).unsqueeze(1)\n",
    "            r_batch = torch.tensor(r_batch, device=device).unsqueeze(1)\n",
    "            d = torch.tensor(d, device=device).unsqueeze(1)\n",
    "            \n",
    "            # Compute Q-values\n",
    "            q = net(s).gather(1, a_batch)\n",
    "            \n",
    "            # Compute target\n",
    "            with torch.no_grad():\n",
    "                nq = target(ns).max(1)[0].unsqueeze(1)\n",
    "                tgt = r_batch + 0.99 * nq * (1 - d)\n",
    "            \n",
    "            # Update network\n",
    "            loss = F.mse_loss(q, tgt)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        # Update target network\n",
    "        if t % 10000 == 0:\n",
    "            target.load_state_dict(net.state_dict())\n",
    "        \n",
    "        # Update progress bar\n",
    "        if t % 50000 == 0:\n",
    "            recent = np.mean(rewards_history[-20:]) if rewards_history else 0\n",
    "            \n",
    "            if len(all_metrics) >= 20:\n",
    "                recent_metrics = all_metrics[-20:]\n",
    "                avg_lifetime = np.mean([m['lifetime'] for m in recent_metrics])\n",
    "                avg_pellet_eff = np.mean([m['pellet_efficiency'] for m in recent_metrics])\n",
    "                \n",
    "                bar.set_postfix({\n",
    "                    \"eps\": f\"{eps(t):.2f}\",\n",
    "                    \"reward\": f\"{recent:.0f}\",\n",
    "                    \"life\": f\"{avg_lifetime:.0f}\",\n",
    "                    \"p_eff\": f\"{avg_pellet_eff:.3f}\"\n",
    "                })\n",
    "            else:\n",
    "                bar.set_postfix({\"eps\": f\"{eps(t):.2f}\", \"reward\": f\"{recent:.0f}\"})\n",
    "    \n",
    "    bar.close()\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    results = {\n",
    "        'seed': seed,\n",
    "        'net_state_dict': net.state_dict(),\n",
    "        'rewards_history': rewards_history,\n",
    "        'all_metrics': all_metrics,\n",
    "        'total_steps': total_steps,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'training_seed_{seed}.pkl')\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    print(f\"\\nSaved training results to {save_path}\")\n",
    "    \n",
    "    return net, rewards_history, all_metrics\n",
    "\n",
    "\n",
    "def run_full_experiment(env_name=\"MsPacman\", \n",
    "                       num_seeds=5, \n",
    "                       training_steps=1_000_000,\n",
    "                       eval_episodes=100,\n",
    "                       save_dir='results'):\n",
    "    \"\"\"Run complete experiment: multiple training runs + evaluation\"\"\"\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = {\n",
    "        'training': [],\n",
    "        'evaluation': []\n",
    "    }\n",
    "    \n",
    "    seeds = [1, 42, 123, 456, 789][:num_seeds]\n",
    "    \n",
    "    for seed in seeds:\n",
    "        # Train\n",
    "        net, rewards, metrics = train_with_seed(\n",
    "            env_name, \n",
    "            seed=seed, \n",
    "            total_steps=training_steps,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        \n",
    "        all_results['training'].append({\n",
    "            'seed': seed,\n",
    "            'rewards': rewards,\n",
    "            'metrics': metrics\n",
    "        })\n",
    "        \n",
    "        # Evaluate\n",
    "        from evaluate import evaluate_agent\n",
    "        \n",
    "        eval_metrics = evaluate_agent(\n",
    "            net, \n",
    "            env_name, \n",
    "            num_episodes=eval_episodes,\n",
    "            base_seed=seed * 1000,\n",
    "            deterministic=True\n",
    "        )\n",
    "        \n",
    "        all_results['evaluation'].append({\n",
    "            'train_seed': seed,\n",
    "            'eval_metrics': eval_metrics\n",
    "        })\n",
    "        \n",
    "        # Save evaluation\n",
    "        eval_save_path = os.path.join(save_dir, f'evaluation_seed_{seed}.pkl')\n",
    "        with open(eval_save_path, 'wb') as f:\n",
    "            pickle.dump(eval_metrics, f)\n",
    "    \n",
    "    # Save combined results\n",
    "    final_save_path = os.path.join(save_dir, 'all_results.pkl')\n",
    "    with open(final_save_path, 'wb') as f:\n",
    "        pickle.dump(all_results, f)\n",
    "    \n",
    "    print(f\"\\nExperiment complete! Results saved to {save_dir}\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f031fd-aeec-4a74-a15f-9549df2e1f74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
